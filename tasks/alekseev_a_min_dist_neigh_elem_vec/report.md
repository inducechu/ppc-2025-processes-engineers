# Нахождение наиболее близких соседних элементов вектора

- **Студент:** Алексеев Артемий Алексеевич, группа 3823Б1ПР2
- **Технология:** SEQ | MPI
- **Вариант:** №7

## 1. Введение

**Цель работы:** Реализовать алгоритм нахождения наиболее близких соседних элементов вектора, далее именуемая НБСЭВ, двумя способами и провести анализ их производительностей.

**Задачи:**
1. Реализовать последовательную версию алгоритма нахождения НБСЭВ.
2. Реализовать параллельную версию с использованием технологии MPI нахождения НБСЭВ.
3. Провести сравнительный анализ производительности и эффективности обеих реализаций.

## 2. Постановка задачи

**Задача**: Найти НБСЭВ.

**Описание метода решения:** Будем проходиться по вектору элементов, находя их модульные разности, и запоминать минимальное значение и индекс первого соседа.
При возникновении ситуации, когда имеем больше число пар ближайших соседей, берем соседей с минимальным индексом.

**Входные данные:**
- `vec` — входной вектор

**Выходные данные:**
- (index, index + 1) - пара соседей, ближайших друг к другу и имеющих минимальные индексы

**Ограничения:** 
- Не предполагаются


## 3. Описание алгоритма (последовательного)
**Алгоритм последовательного вычисления:**
1. **Пройдемся по циклу от 0 до size - 1**.
2. **Найдем модульную разность соседей** `|vec[i] - vec[i + 1]|`.
3. **При условии, что разность соседей меньше, чем в буфере, перезаписываем** ` index = i; index_value = value;`.

**Реализация на C++:**

```cpp
int index = 0;
int index_value = std::numeric_limits<int>::max();
for(int i = 0; i < static_cast<int>(vec.size()) - 1; i++){
  int value = std::abs(vec[i] - vec[i + 1]);
  if(value < index_value){
    index = i;
    index_value = value;
  }
}
```
## 4. Схема распараллеливания (MPI)
**Алгоритм параллельного вычисления:**
Аналогично, с некоторыми дополнениями 
1. **Каждый процесс начинает с начальным шагом равным его рангу в цикле и двигается со step = кол-во процессов:** `for(int i = rank; i < static_cast<int>(vec.size()) - 1; i += comm_size)`.
2. **Каждый процесс ищет свой минимум в его ресурсах**.
3. **Глобальная длина формируется из минимумов каждого локального минимума процесса:** `MPI_Allreduce(&local_dist, &global_dist, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);`.
4. **Финальный расчет: также при одинаковых минимальных дистанциях ищем по минимальному индексу** `MPI_Allreduce(&global_index, &finish_global_index, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);`
**Принцип разделения отрезков:**
```cpp
for(int i = rank; i < static_cast<int>(vec.size()) - 1; i += comm_size){
  int value = std::abs(vec[i] - vec[i + 1]);
  if(local_dist > value){
    local_dist = value;
    local_index = i;
  }
}
```

**Схема распределения вычислений:**
- Процесс 0: обрабатывает отрезки 0, size, 2×size, ...
- Процесс 1: обрабатывает отрезки 1, size+1, 2×size+1, ...
- Процесс k: обрабатывает отрезки k, size+k, 2×size+k, ...

## 5. Детали реализации
### 5.1. Структура реализации
```text
alekseev_a_min_dist_neigh_elem_vec
    ├───common
    │   └───include
    │           common.hpp - определение типов входных/выходных/тестовых данных
    │
    ├───mpi
    │   ├───include
    │   │       ops_mpi.hpp - заголовочный файл MPI-реализации
    │   │
    │   └───src
    │           ops_mpi.cpp - код MPI-реализации
    │
    ├───seq
    │   ├───include
    │   │       ops_seq.hpp - заголовочный файл SEQ-реализации
    │   │
    │   └───src
    │           ops_seq.cpp - код SEQ-реализации
    │
    └───tests
        ├───functional
        │       main.cpp - функциональные тесты
        │
        └───performance
                main.cpp - тесты производительности
```
### 5.2. Основные классы / функции
- `AlekseevAMinDistNeighElemVecSEQ` - последовательная реализация
- `AlekseevAMinDistNeighElemVecMPI` - параллельная реализация
### 5.3. Пространственная и временная сложности алгоритмов
- Последовательная версия:
    - Временная сложность - `O(N)`, где `N` - размер входного вектора;
    - Пространственная сложность - `O(N)`, где `N` - размер входного вектора.
- Параллельная версия:
    - Временная сложность - `O(N/P + MP)`, где `N` - размер входного вектора, `P` - количество процессов. Слагаемое `N/P` - сложность локальных вычислений, `MP` - сложность операции `MPI`;
    - Пространственная сложность: `O(N/P)`, где `N` - размер входного вектора, `P` - количество процессов.
## 6. Экспериментальная среда
### 6.1. Аппаратное обеспечение:
| Параметр | Значение                                                                               |
| -------- | -------------------------------------------------------------------------------------- |
| CPU      | Ryzen 5 5600                                                                           |
| RAM      | 16 GB DDR4                                                                             |     
### 6.2. Программное обеспечение:
| Параметр   | Значение                                               |
| ---------- | ------------------------------------------------------ |
| ОС         | Windows 11 Home + WSL                                  |
| MPI        | OpenMPI 3.1                                            |
| Компилятор | g++ 14.2.0                                             |
| Сборка     | Release                                                |
### 6.3. Тестовые данные
**Функциональные тесты:**\
Используют заранее подготовленный массив пар `вектор - описание тестового случая`. Корректность ответа проверяется в функции `CheckTestOutputData` с помощью вычисления ответа последовательным алгоритмом.
**Тесты производительности:**
Вектор размером 100000000 генерируется в цикле по индексам вектора с помощью `(i % 1000 + 7) * 3`
## 7. Результаты и обсуждение

### 7.1 Корректность
Проверка корректности реализована средствами Google Test:
- 20 функциональных тестов покрывают различные ситуации.
- Perf-тест проверяет, что результат валиден.

Реализация успешно проходит как функциональные тесты, так и перф-тесты.

### 7.2 Производительность
**Метрики:**
1. Абсолютное время выполнения вычислительной части алгоритма в миллисекундах;
2. Ускорение относительно последовательной версии;
3. Эффективность распараллеливания = `(ускорение / число процессов) * 100%`.

**Полученные результаты:**

| **Режим** | **Количество процессов** | **Время, с** | **Speedup** | **Efficiency** |
|-----------|--------------------------|--------------|-------------|----------------|
| SEQ       | 1                        | 0.193        | 1.00        | N/A            |
| MPI       | 2                        | 0.114        | 1.69        | 84,5%          |
| MPI       | 4                        | 0.056        | 3.44        | 86%            |

## 8. Заключение
В рамках данной работы были успешно реализованы алгоритмы нахождения наиболее близких соседних элементов вектора. Проведенные эксперименты подтвердили значительное ускорение MPI-реализации и корректность работы обоих вариантов алгоритма.

## 9. Источники
1. [Презентация по курсу](https://learning-process.github.io/parallel_programming_slides/)
